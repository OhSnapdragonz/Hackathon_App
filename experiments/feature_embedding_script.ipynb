{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "files_objs = os.listdir(os.path.join(\"C:\\\\Users\\CLO\\OneDrive - Stryker\\\\School\\\\Qualcomm Hackathon\", \"cards_image\"))\n",
    "files_objs = [os.path.join(\"C:\\\\Users\\\\CLO\\\\OneDrive - Stryker\\\\School\\\\Qualcomm Hackathon\", \"cards_image\", f) for f in files_objs]\n",
    "\n",
    "def output_images(similar, target, n=5):\n",
    "    display_img(target, \"original\")\n",
    "    counter = 0\n",
    "    for k, v in similar.items():\n",
    "        display_img(k, \"similarity:\" + str(v))\n",
    "        counter += 1\n",
    "        if counter == n:\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "def display_img(path, title):\n",
    "    plt.imshow(Image.open(path))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and get embeddings from dataset\n",
    "\n",
    "# Load model (e.g., ResNet18)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove the classification head\n",
    "model.eval()\n",
    "\n",
    "# define preprocessing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Get embeddings\n",
    "dataset = {}\n",
    "for file in files_objs:\n",
    "    image = Image.open(file)\n",
    "    # Check if the image has an alpha channel (RGBA)\n",
    "    if image.mode == 'RGBA' or image.mode == 'P':\n",
    "        # Remove the alpha channel by converting to RGB\n",
    "        image = image.convert('RGB')\n",
    "    print(file)\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        vector = model(input_tensor).flatten()\n",
    "    dataset[str(file)] = vector\n",
    "\n",
    "data = {\"model\": \"resnet18\", \"embeddings\": dataset}\n",
    "torch.save(\n",
    "    data, \"resnet18_embeddings.pt\"\n",
    ")  # need to update functionality for naming convention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to get image embeddings\n",
    "model = models.resnet18(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove the classification head\n",
    "model.eval()\n",
    "\n",
    "# define preprocessing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "target = \"test_embedding3.jpg\"\n",
    "image = Image.open(target)\n",
    "input_tensor = transform(image).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    target_vector = model(input_tensor).flatten().unsqueeze(0)\n",
    "\n",
    "n = None\n",
    "# initiate computation of consine similarity\n",
    "cosine = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "# iteratively store similarity of stored images to target image\n",
    "sim_dict = {}\n",
    "for k, v in dataset.items():\n",
    "    sim = cosine(v.unsqueeze(0), target_vector)[0].item()\n",
    "    sim_dict[k] = sim\n",
    "\n",
    "# sort based on decreasing similarity\n",
    "items = sim_dict.items()\n",
    "sim_dict = {k: v for k, v in sorted(items, key=lambda i: i[1], reverse=True)}\n",
    "\n",
    "# cut to defined top n similar images\n",
    "if n is not None:\n",
    "    sim_dict = dict(list(sim_dict.items())[: int(n)])\n",
    "\n",
    "output_images(sim_dict, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
